#!/bin/bash

function display_usage()
{
  echo "Usage : $0 -b (benchmark tool) <sysbench, pgbench or hammerdb> -j (job type) <prepare, run or cleanup(sysbench only)> -c (config file) -n (run name)"
}

function mydate()
{
  date +%Y%m%d:%H:%M:%S:
}

while getopts b:j:c:n:h flag
do
    case "${flag}" in
        b) readonly BENCHMARK_TOOL=${OPTARG};;
        j) readonly JOB_TYPE=${OPTARG};;
        c) readonly CONFIG_FILE=${OPTARG};;
        n) readonly RUN_NAME=${OPTARG};;
        h) display_usage;exit;;
    esac
done

if [[ -z ${BENCHMARK_TOOL} || -z ${JOB_TYPE} ]]; then
  display_usage
  exit 1
fi

[[ -z ${CONFIG_FILE} ]] && CONFIG_FILE=sherlock.config

if [[ -z ${RUN_NAME} ]]; then
  if [ "$(uname -s)" == "Linux" ];then
    RUN_NAME=$(cat /dev/urandom | tr -dc "[:lower:]" | head -c 6)
  else
    RUN_NAME=$(cat /dev/urandom | LC_CTYPE=C tr -dc "[:lower:]" | head -c 6)
  fi
fi

source ${CONFIG_FILE}

if [[ ! -s ${WORKERS_LIST_FILE} || ! -s ${SDS_LIST_FILE} ]]; then
  echo "$(mydate) worker or sds file are either missing or empty..."
  exit 1
fi

if [[ "${BENCHMARK_TOOL}" == "hammerdb" ]]; then
  readonly STATS_COUNT=$(((HAMMERDB_RAMPUP+WORKLOAD_RUNTIME+10)/STATS_INTERVAL))
else
  readonly STATS_COUNT=$(((WORKLOAD_RUNTIME+10)/STATS_INTERVAL))
fi

declare -A jobs_pods
declare -A worker_local_device


if [ "$(uname -s)" == "Linux" ];then
  if [[ "${WORKERS_LIST_FILE}" == "${SDS_LIST_FILE}" ]]; then
    mapfile -t sds_node_array < <(cat ${WORKERS_LIST_FILE})
  else
    mapfile -t worker_node_array < <(cat ${WORKERS_LIST_FILE})
    mapfile -t sds_node_array < <(cat ${SDS_LIST_FILE})
  fi
else
  if [[ "${WORKERS_LIST_FILE}" == "${SDS_LIST_FILE}" ]]; then
    declare -a worker_node_array
    while IFS= read -r line; do sds_node_array+=("$line"); done < <(cat ${WORKERS_LIST_FILE})
  else
    declare -a worker_node_array
    while IFS= read -r line; do worker_node_array+=("$line"); done < <(cat ${WORKERS_LIST_FILE})
    declare -a sds_node_array
    while IFS= read -r line; do sds_node_array+=("$line"); done < <(cat ${SDS_LIST_FILE})
  fi
fi


function get_db_pvc_mapping()
{
  declare -A pvc_array
  if [[ "${STORAGE_VENDOR}" == "ceph" ]]; then
    for ((j=0; j<=${DB_PER_WORKER}*${NUMBER_OF_WORKERS}-1; j++))
    do
      deployment_name=${DB_POD_PREFIX}-${j}
      claim_name=$(${KUBE_CMD} -n ${NAMESPACE_NAME} get deployment ${deployment_name} -o jsonpath='{.spec.template.spec.volumes[].persistentVolumeClaim.claimName}')
      volume_name=$(${KUBE_CMD} -n ${NAMESPACE_NAME} get pvc ${claim_name} --no-headers -o custom-columns=":spec.volumeName")
      pvc_array[${j}]=${volume_name}
    done
    for node_name in $(cat $WORKERS_LIST_FILE)
    do
      local host_name=$(${KUBE_CMD} get node ${node_name} -o jsonpath='{.metadata.labels.kubernetes\.io/hostname}')
      rbd_line=""
      node_pvc=$(oc debug node/${node_name} -- lsblk | grep rbd | awk '{print $1"@@"$7}')
      for lsblk_line in $(echo ${node_pvc})
      do
        pvc="pvc-$(echo $lsblk_line | awk -F'@@' '{print $2}' | awk -F"pvc-" '{print $2}' | tr -d '/mount')"
        for pvc_volume in ${pvc_array[@]}
        do
          if [[ "${pvc}" == "${pvc_volume}" ]]; then
            rbd_line+=" $(echo $lsblk_line | awk -F'@@' '{print $1}')"
          fi
        done
      done
      rbd_line=${rbd_line#?}
      worker_local_device[${host_name}]=${rbd_line}
    done
  fi
  if [[ "${STORAGE_VENDOR}" == "lightbits" ]]; then
    for ((j=0; j<=${DB_PER_WORKER}*${NUMBER_OF_WORKERS}-1; j++))
    do
      deployment_name=${DB_POD_PREFIX}-${j}
      deployment_data=$(${KUBE_CMD} -n ${NAMESPACE_NAME} get deployment ${deployment_name} -o jsonpath='{.spec.template.spec.containers[].volumeMounts[].mountPath}{","}{.spec.template.spec.nodeSelector.kubernetes\.io/hostname}')
      mount_point=$(echo ${deployment_data} | awk -F, '{print $1}')
      k8s_hostname=$(echo ${deployment_data} | awk -F, '{print $2}')
      pod_name=$(${KUBE_CMD} -n ${NAMESPACE_NAME} get pod -l app=${deployment_name} -o jsonpath="{.items[0].metadata.name}")
      db_local_device=$(${KUBE_CMD} exec -n ${NAMESPACE_NAME} ${pod_name} -- /bin/bash -c "lsblk" | grep ${mount_point} | awk '{print $1}')
      worker_local_device[${k8s_hostname}]+=" ${db_local_device}"
    done
  fi
}

function prepare_run_command()
{
  if [[ "${BENCHMARK_TOOL}" == "sysbench" ]]; then
    run_command="./run_sysbench ${JOB_TYPE} ${WORKLOAD_RUNTIME} ${THREADS} ${SYSBENCH_READ_ONLY} ${SYSBENCH_WRITE_ONLY} ${db_ip} ${OUTPUT_INTERVAL} ${SYSBENCH_ROWS_IN_TABLE} ${SYSBENCH_NUMBER_OF_TABLES} ${DB_TYPE} ${DB_USERNAME} ${DB_PASSWORD} ${DB_NAME} ${SYSBENCH_NUMBER_OF_INSERTS} ${SYSBENCH_NUMBER_OF_UPDATES} ${SYSBENCH_NUMBER_OF_NON_INDEX_UPDATES}"
  fi
  if [[ "${BENCHMARK_TOOL}" == "pgbench" ]]; then
    if [[ "${JOB_TYPE}" == "prepare" ]]; then
      run_command="./run_pgbench ${JOB_TYPE} ${db_ip} ${CLIENTS} ${THREADS} ${PGBENCH_RUN_TYPE} ${PGBENCH_RUN_TYPE_VAR} ${PGBENCH_VACUUM} ${PGBENCH_QUIET} ${PGBENCH_SCALE} ${DB_USERNAME} ${DB_PASSWORD} ${DB_NAME} ${OUTPUT_INTERVAL} ${PGBENCH_READ_ONLY}"
    else
      run_command="./run_pgbench ${PGBENCH_WORKLOAD_TYPE} ${db_ip} ${CLIENTS} ${THREADS} ${PGBENCH_RUN_TYPE} ${PGBENCH_RUN_TYPE_VAR} ${PGBENCH_VACUUM} ${PGBENCH_QUIET} ${PGBENCH_SCALE} ${DB_USERNAME} ${DB_PASSWORD} ${DB_NAME} ${OUTPUT_INTERVAL} ${PGBENCH_READ_ONLY}"
    fi
  fi
  if [[ "${BENCHMARK_TOOL}" == "hammerdb" ]]; then
    case "${JOB_TYPE}" in
      prepare)
        run_command="./run_hammerdb mssqls prepare ${db_ip} ${SA_PASSWORD} ${HAMMERDB_WAREHOUSES} ${HAMMERDB_BUILD_VU}"
        ;;
      run)
        run_command="./run_hammerdb mssqls run ${db_ip} ${SA_PASSWORD} ${HAMMERDB_WAREHOUSES} ${HAMMERDB_RUN_VU} ${OUTPUT_INTERVAL} $((HAMMERDB_RAMPUP/60)) $((WORKLOAD_RUNTIME/60)) ${HAMMERDB_RAISE_ERROR}"
        ;;
      cleanup)
        run_command="./run_hammerdb mssqls cleanup ${db_ip} ${SA_PASSWORD} ${HAMMERDB_WAREHOUSES} ${HAMMERDB_RUN_VU}"
        ;;
    esac
  fi
  if [[ "${BENCHMARK_TOOL}" == "ycsb" ]]; then
    run_command="./run_ycsb ${DB_TYPE} ${JOB_TYPE} ${db_ip} ${YCSB_WORKLOAD} ${THREADS} ${DB_USERNAME} ${DB_PASSWORD} ${DB_NAME} ${YCSB_RECORDCOUNT} ${YCSB_OPERATIONCOUNT} ${YCSB_DISTRIBUTION} ${YCSB_READPROPORTION} ${YCSB_UPDATEPROPORTION} ${WORKLOAD_RUNTIME}"
  fi
  if ${DEBUG}; then echo "$(mydate)-run_command=${run_command}";fi
}

function start_the_workload_job()
{
  local node_name=$(${KUBE_CMD} get node ${node_name} -o jsonpath='{.metadata.labels.kubernetes\.io/hostname}')
  local db_ip="${1}"
  prepare_run_command
  job_name=$(cat <<EOF | ${KUBE_CMD} create -f -
apiVersion: batch/v1
kind: Job
metadata:
  generateName: ${BENCHMARK_TOOL}-${JOB_TYPE}-${deployment_name}-${RUN_NAME}-
  namespace: ${NAMESPACE_NAME}
spec:
  template:
    spec:
      restartPolicy: OnFailure
      nodeSelector:
        kubernetes.io/hostname: ${node_name}
      containers:
        - image: quay.io/sagyvolkov/benchmark-container:sherlock0.5
          imagePullPolicy: Always
          name: ${BENCHMARK_TOOL}-${JOB_TYPE}
          resources:
            limits:
              memory: ${BENCHMARK_POD_MEM_LIMIT}
              cpu: ${BENCHMARK_POD_CPU_LIMIT}
          command:
                - "bash"
                - "-c"
                - >
                  ${run_command}
EOF
)
job_name=$(echo ${job_name} | awk '{print $1}')
workload_pod_name=$(${KUBE_CMD} -n ${NAMESPACE_NAME} describe ${job_name} | grep "Created pod:" | awk -F "Created pod: " '{print $2}')
}

function find_taints()
{
 local node_name="${1}"
 local node_taint=$(${KUBE_CMD} get node ${node_name} -o jsonpath='{.spec.taints[0].effect}{"ZZZ"}{.spec.taints[0].key}{"ZZZ"}{.spec.taints[0].value}')
 if [[ ${node_taint} != "ZZZZZZ" ]];then
   toleration_effect=$(echo "${node_taint}" | awk -F"ZZZ" '{print $1}')
   toleration_key=$(echo "${node_taint}" | awk -F"ZZZ" '{print $2}')
   toleration_value=$(echo "${node_taint}" | awk -F"ZZZ" '{print $3}')
   if [[ "${toleration_value}" == "" ]]; then
     toleration=$(echo -e "tolerations:\n      - key: \"${toleration_key}\"\n        operator: \"Exists\"\n        effect: ${toleration_effect}")
   else
     toleration=$(echo -e "tolerations:\n      - key: \"${toleration_key}\"\n        value: \"${toleration_value}\"\n        effect: ${toleration_effect}")
   fi
 fi
 
 echo "${toleration}"
}

function run_stats()
{
#        - image: quay.io/sagyvolkov/stats-container:sherlock0.5
  local toleration=""
  local node_type="${1}"
  if [[ "${SDS_NODE_TAINTED}" == "true" && "${node_type}" == "sds" ]]; then
    echo "$(mydate) Checking for tolerations in sds node ${node_name}..."
    toleration=$(find_taints ${node_name})
  fi  
  local node_name=$(${KUBE_CMD} get node ${node_name} -o jsonpath='{.metadata.labels.kubernetes\.io/hostname}')
  network_interfaces="${NODE_NETWORK_MAP[${node_name}]}"
  job_name=$(cat <<EOF | ${KUBE_CMD} create -f -
apiVersion: batch/v1
kind: Job
metadata:
  generateName: stats-${node_type}-${RUN_NAME}-${node_name}-
  namespace: ${NAMESPACE_NAME}
spec:
  template:
    spec:
      ${toleration}
      hostNetwork: true
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/hostname: ${node_name}
      containers:
        - image: quay.io/sagyvolkov/stats-container:sherlock0.7
          name: stats-${RUN_NAME}
          resources:
            limits:
              memory: 64Mi
              cpu: 0.1
          command:
              - "bash"
              - "-c"
              - echo "${network_interfaces}" > /tmp/network_interfaces; echo "${SDS_DEVICES}" > /tmp/sds_devices; ./run_all_scripts ${STATS_INTERVAL} ${STATS_COUNT} ${node_type}
EOF
)
job_name=$(echo ${job_name} | awk '{print $1}')
stats_pod_name=$(${KUBE_CMD} -n ${NAMESPACE_NAME} describe ${job_name} | grep "Created pod:" | awk -F "Created pod: " '{print $2}')
}

function stats_collect()
{
  local node_type=${1}
  shift 1
  local local_array=("$@")
  for node_number in ${!local_array[@]}
  do
    node_name=${local_array[${node_number}]}
    echo "$(mydate) Starting to collect stats on ${node_type} node ${node_name}..."
    #run_stats "${node_type}" "${local_array[@]}"
    run_stats "${node_type}"
    jobs_pods[${job_name}]=${stats_pod_name}
  done
}

function calculate_local_device_utilization()
{
  local file_name=${1}
  local local_device="${2}"
  for device_name in ${local_device}
  do
    correct_device="${device_name:0:5}c.${device_name:5}"
    let r_s=0
    let w_s=0
    let util=0
    let number_of_inputs=0
    while read x
    do
      r_s=$(echo "$r_s + $(echo ${x}|awk -F, '{print $1}')"|bc)
      w_s=$(echo "$w_s + $(echo ${x}|awk -F, '{print $2}')"|bc)
      util=$(echo "$util + $(echo ${x}|awk -F, '{print $3}')"|bc)
      let number_of_inputs++
    done < <(grep ${correct_device} ${file_name} | awk '{print $2","$8","$23}')
    avg_r_s=$(echo "scale=2;$r_s/$number_of_inputs"|bc)
    avg_w_s=$(echo "scale=2;$w_s/$number_of_inputs"|bc)
    avg_util=$(echo "scale=2;$util/$number_of_inputs"|bc)
    echo "SUMMARY: ${node_name}, averages for local device ${device_name} read/s: ${avg_r_s}, write/s: ${avg_w_s}, utilization%: ${avg_util}" >> ${file_name}
  done
}

if [[ "${JOB_TYPE}" == "run" && "${STATS}" == "true" ]]; then
  [[ "${KUBE_CMD}" == "oc" ]] && oc adm policy add-scc-to-user hostnetwork -n${NAMESPACE_NAME} -z default
  if [[ "${WORKERS_LIST_FILE}" == "${SDS_LIST_FILE}" ]]; then
    [[ ${#sds_node_array[@]} -ne 0 ]] && stats_collect sds ${sds_node_array[@]}
  else
    stats_collect worker ${worker_node_array[@]}
    if [[ "${STORAGE_TYPE}" == "internal" ]]; then
      [[ ${#sds_node_array[@]} -ne 0 ]] && stats_collect sds ${sds_node_array[@]}
    fi
  fi
fi

for ((j=0; j<=${DB_PER_WORKER}*${NUMBER_OF_WORKERS}-1; j++))
do
  node_number=$((j%NUMBER_OF_WORKERS))
  if [[ "${WORKERS_LIST_FILE}" == "${SDS_LIST_FILE}" ]]; then
    node_name=${sds_node_array[${node_number}]}
  else
    node_name=${worker_node_array[${node_number}]}
  fi
  deployment_name=${DB_POD_PREFIX}-${j}
  database_ip=$(${KUBE_CMD} get svc -n ${NAMESPACE_NAME} ${deployment_name} -o jsonpath='{.spec.clusterIP}')
  echo "$(mydate) Starting ${BENCHMARK_TOOL} job for ${JOB_TYPE} in deployment ${deployment_name} with database ip ${database_ip} ..."
  start_the_workload_job ${database_ip}
  jobs_pods[${job_name}]=${workload_pod_name}
  echo "$(mydate) ${job_name} is using ${BENCHMARK_TOOL} pod ${workload_pod_name} on node ${node_name}"
done

if [[ "${JOB_TYPE}" == "run" ]];then
  if [[ -z ${RUN_NAME} ]]; then
    RUN_NAME="${BENCHAMRK_TOOL}-${JOB_TYPE}-$(date +%s)"
  fi
  if [[ ! -d "${RUN_NAME}" ]]; then
    mkdir -p "$RUN_NAME"
  fi

  cp ${CONFIG_FILE} ${RUN_NAME}/
  cp ${WORKERS_LIST_FILE} ${RUN_NAME}/
  cp ${SDS_LIST_FILE} ${RUN_NAME}/
  job_list=""

  for i in "${!jobs_pods[@]}"
  do
    job_list="${job_list} ${i}"
  done
  echo "$(mydate) Waiting for jobs to complete ..."
  ${KUBE_CMD} -n ${NAMESPACE_NAME} wait --for=condition=complete ${job_list} --timeout=4000s

  echo "$(mydate) All jobs are done, getting logs ..."

  for i in "${!jobs_pods[@]}"
  do
    ${KUBE_CMD} -n ${NAMESPACE_NAME} logs ${jobs_pods[$i]} > ${RUN_NAME}/${jobs_pods[$i]}.log
  done

  #if [[ "${STATS}" == "true" && "${PVC_STATS}" == "true" && "${KUBE_CMD}" == "oc" ]]; then
  if [[ "${STATS}" == "true" && "${PVC_STATS}" == "true" ]]; then
    echo "$(mydate) Generating list of database:PVC:PV:local device from worker nodes and pods ..."
    get_db_pvc_mapping
    for node_name in ${!worker_local_device[@]}
    do
      if [[ "${WORKERS_LIST_FILE}" == "${SDS_LIST_FILE}" ]]; then
        file_name=$(ls ${RUN_NAME}/stats-sds-${RUN_NAME}-${node_name}*.log)
      else
        file_name=$(ls ${RUN_NAME}/stats-worker-${RUN_NAME}-${node_name}*.log)
      fi
      local_device="${worker_local_device[${node_name}]}"
      calculate_local_device_utilization ${file_name} "${local_device}"
    done
  fi
else
  echo "$(mydate) Jobs are running in the background. Use \"${KUBE_CMD} get jobs\" to monitor the jobs..."
fi
echo "$(mydate) End of $0 script ..."
